---
title: "England Peat Depth Model: Train models"
output: html_notebook
---

#Peat Depths: run models

## Packages 
```{r, include=FALSE}
library(raster)
library(rgdal)

library(randomForest)
library(Rborist)

library(caret)
library(parallel)
library(doParallel)
```

## Load data
```{r}
load(file = "../data/input.data.rds")
dim(input.data)
```


## Start Parallel Processing
```{r}
detectCores()
getDoParWorkers()
cl <- makeCluster(detectCores()-1)
registerDoParallel(cl)
getDoParName()
getDoParWorkers()
#registerDoSEQ() # to stop parallel processing
```


## Make training and test sets

```{r}
set.seed(123)
inTrain <- createDataPartition(y = input.data$PEAT_DEPTH, 
                               p = .75, # The percentage of data in the training set
                               list = FALSE) # The format of the results

training <- input.data[inTrain,]
testing <- input.data[-inTrain,]
```
Training data has `r nrow(training)` records.  Testing data has `r nrow(testing)` records.  

#### train model
Need the latest version of caret from github to avoid known bug.

set model parameters
```{r}
params <- trainControl(method = "cv", number = 10)
```

train linear model
```{r}
Mlm2a <- train(PEAT_DEPTH ~ ., 
              data = training, 
              method = "lm")

save(Mlm2a, file = "../data/models/Mlm2a.rds")

Mlm2a
```

train model
```{r}
Mrf2a <- train(PEAT_DEPTH ~ ., 
              data = training, 
              method = "Rborist")

save(Mrf2a, file = "../data/models/Mrf2a.rds")

Mrf2a
```


```{r}
rbind(getTrainPerf(Mlm2a), getTrainPerf(Mrf2a))
```


## Evaluate models


residuals????
```{r}
pd <- data.frame(residuals(Mlm2a))
names(pd) <- "resid.Mlm2a"
brks <- quantile(pd$resid.Mlm2a, na.rm=TRUE)

hist(pd$resid.Mlm2a)
```



```{r}
cvValues <- resamples(list(LM = Mlm2a, RF = Mrf2a))
summary(cvValues)
splom(cvValues, metric = "Rsquared")
parallelplot(cvValues, metric = "Rsquared")
dotplot(cvValues, metric = "Rsquared")
```

```{r}
Diffs <- diff(cvValues, metric = "Rsquared")
summary(Diffs)
dotplot(Diffs, metric = "Rsquared")
```

## predict test data from model and evaluate

```{r}
lm_pred <- predict(Mlm2a, training)

rf_pred <- predict(Mrf2a, training)
```

Calculate deviation

**RMSE**: Root Mean Square Error. Standard deviation of the differences between predicted and observed values.  Measures accuracy.  Scale-dependent, so can't compare different datasets.  Sensitive to outliers. Smaller is better.  Calculated by   `sqrt(mean((pred - obs)^2` 

**Rsquared**: Coefficient of determination.  The proportion of the variation in the observed variable that is predictable from the predicted variable.  Not scale dependent.  Closer to 1 is better. Calculated by `R^2 = 1-\frac{∑ (y_i - \hat{y}_i)^2}{∑ (y_i - \bar{y}_i)^2}`

**MAE**: Mean Absolute Error.  The average of the difference between the absolute predicted and observed values.  Scale-dependent, so can't compare different datasets.  Sensitive to outliers. Smaller is better.  Calculated by `mean(abs(pred - obs))`


```{r}
postResample(pred = lm_pred, obs = training$PEAT_DEPTH)
postResample(pred = rf_pred, obs = training$PEAT_DEPTH)
```

## Train full models

```{r}
Mlm2a.full <- train(PEAT_DEPTH ~ ., 
              data = input.data, 
              method = "lm")

save(Mlm2a.full, file = "../data/models/Mlm2a.full.rds")

Mlm2a.full
```

train model
```{r}
Mrf2a.full <- train(PEAT_DEPTH ~ ., 
              data = input.data, 
              method = "Rborist")

save(Mrf2a.full, file = "../data/models/Mrf2a.full.rds")

Mrf2a.full
```

evaluate
```{r}
rbind(getTrainPerf(Mlm2a.full), getTrainPerf(Mrf2a.full))
```

